{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Useless Notebook - Only used for testing random bits of code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.image import grid_to_graph\n",
    "from skimage.segmentation import slic\n",
    "from skimage.util import img_as_float\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "def image_to_graph(image_path):\n",
    "    # Load and normalize the image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = img_as_float(image)\n",
    "    \n",
    "    # Segment the image using SLIC\n",
    "    segments = slic(image, n_segments=100, compactness=10, sigma=3)\n",
    "    \n",
    "    # Create a graph from the segmented image\n",
    "    graph = grid_to_graph(*segments.shape)\n",
    "    \n",
    "    # Extract features for each segment (mean color here)\n",
    "    features = []\n",
    "    for segment_id in np.unique(segments):\n",
    "        mask = segments == segment_id\n",
    "        features.append(np.mean(image[mask], axis=0))\n",
    "    features = np.array(features)\n",
    "    \n",
    "    # Adjust edge_index to ensure indices are within bounds\n",
    "    max_node_idx = features.shape[0] - 1\n",
    "    graph.row = np.clip(graph.row, 0, max_node_idx)\n",
    "    graph.col = np.clip(graph.col, 0, max_node_idx)\n",
    "    \n",
    "    return graph, features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vshra\\AppData\\Local\\Temp\\ipykernel_8692\\3838640646.py:18: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  edge_index = torch.tensor(graph.nonzero(), dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import torch\n",
    "import os\n",
    "\n",
    "class HistopathologyDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.graphs = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Assuming your directory structure is as described\n",
    "        for label in ['0', '1']:\n",
    "            label_dir = os.path.join(root_dir, label)\n",
    "            for image_path in os.listdir(label_dir)[:1000]:\n",
    "                graph, features = image_to_graph(os.path.join(label_dir, image_path))\n",
    "                edge_index = torch.tensor(graph.nonzero(), dtype=torch.long)\n",
    "                x = torch.tensor(features, dtype=torch.float)\n",
    "                y = torch.tensor(int(label), dtype=torch.long)\n",
    "                data = Data(x=x, edge_index=edge_index, y=y)\n",
    "                self.graphs.append(data)\n",
    "                self.labels.append(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.graphs[idx]\n",
    "\n",
    "# Initialize the dataset\n",
    "full_dataset = HistopathologyDataset('Root')\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset)) # Split data count into training and validation splits in the ratio 80% to 20%\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size]) # Split the data\n",
    "\n",
    "batch_size = 32  #Set batch size\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4) \n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=4) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels: int = 3, hidden_channels: int = 152, num_classes: int = 2):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels*4)\n",
    "        self.conv2 = GCNConv(hidden_channels*4, hidden_channels*2)\n",
    "        self.conv3 = GCNConv(hidden_channels*2, hidden_channels)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.conv3(x, edge_index).relu()\n",
    "        x = global_mean_pool(x, batch)  # Pooling\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = GCN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(100):\n",
    "    total_train_loss = 0\n",
    "    total_val_loss = 0\n",
    "\n",
    "    correct_train_predictions = 0\n",
    "    correct_val_predictions = 0\n",
    "\n",
    "    total_train_predictions = 0\n",
    "    total_val_predictions = 0\n",
    "    \n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch.x, batch.edge_index, batch.batch)\n",
    "        _, preds = torch.max(out, dim=1)  # Get the predicted class labels\n",
    "        loss = criterion(out, batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        correct_train_predictions += (preds == batch.y).sum().item()\n",
    "        total_train_predictions += batch.y.size(0)\n",
    "\n",
    "    model.eval()\n",
    "    for batch in val_dataloader:\n",
    "        batch = batch.to(device)\n",
    "        out = model(batch.x, batch.edge_index, batch.batch)\n",
    "        _, preds = torch.max(out, dim=1)  # Get the predicted class labels\n",
    "        loss = criterion(out, batch.y)\n",
    "        total_val_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        correct_val_predictions += (preds == batch.y).sum().item()\n",
    "        total_val_predictions += batch.y.size(0)\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    train_accuracy = correct_train_predictions / total_train_predictions\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    val_accuracy = correct_val_predictions / total_val_predictions\n",
    "    print(f'Epoch {epoch}, Train Loss: {avg_train_loss}, Train Accuracy: {train_accuracy}, Val Loss: {avg_val_loss}, Val Accuracy: {val_accuracy}')\n",
    "\n",
    "\n",
    "# Save the trained model\n",
    "# torch.save(model.state_dict(), 'gcn_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
